# LLM-Finetuning-Playground

A beginner-friendly project for fine-tuning, testing, and deploying language models with a strong emphasis on quality assurance and testing methodologies.

## ðŸŽ¯ What This Project Does

This project demonstrates how to:

- Fine-tune language models for different types of tasks
- Implement proper testing and evaluation methodologies 
- Create evaluation metrics and test sets for consistent model testing
- Deploy models to different platforms (Hugging Face Hub, Ollama)
- Create interactive demos for testing models

## ðŸ¤– Demo Projects

### 1. Sentiment Analysis (Classification)

![Sentiment Analysis Demo](data/img/sentiment-analysis-demo.png)

- Fine-tunes a **DistilBERT** model to classify movie reviews as positive or negative
- Emphasizes classification metrics (accuracy, F1 score, precision, recall)
- Deployed to **Hugging Face Hub**

### 2. Recipe Assistant (Text Generation)

- Fine-tunes a **TinyLlama** model to generate recipes from ingredient lists
- Focuses on generation quality and coherence
- Deployed to **Ollama**

## ðŸ§ª Testing & Quality Assurance Focus

This project places special emphasis on testing methodologies for ML models. For a comprehensive guide to our testing approach, see [GETTING_STARTED_DEMO_1.md#testing-philosophy-and-methodology](GETTING_STARTED_DEMO_1.md#testing-philosophy-and-methodology).

### Test Types Implemented

- **Unit tests**: Testing individual components like data loaders
- **Functional tests**: Testing model predictions with known inputs
- **Performance tests**: Ensuring the model meets accuracy and speed requirements
- **Balanced test sets**: Creating test data with equal class distribution
- **High-confidence evaluations**: Analyzing model confidence in predictions

### Testing Principles

- **Reproducibility**: Tests use fixed test sets to ensure consistent evaluation
- **Isolation**: Components are tested independently
- **Metrics tracking**: F1 score, precision, recall, and accuracy are tracked
- **Performance benchmarking**: Measuring inference speed

## ðŸš€ Getting Started

See the [GETTING_STARTED.md](GETTING_STARTED.md) file for detailed instructions on:

- Setting up your environment
- Training your first model
- Running the test suite
- Using the interactive demo

## ðŸ“Š Example Results

After training, you'll be able to analyze sentiment in text:

# Project WIP Notes
here we will keep notes on the project to assist with the direction of the project for myself and for any AI agent assistance so they have a high level of the project without crawling the file directory.

## Project Structure

```yaml
LLM-Finetuning-Playground/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ default.yaml                  # Base configuration file
â”‚   â”œâ”€â”€ sentiment_analysis.yaml       # Sentiment analysis training configuration
â”‚   â”œâ”€â”€ text_generation.yaml          # Recipe generation base configuration
â”‚   â”œâ”€â”€ text_generation_small.yaml    # Recipe generation with smaller model settings
â”‚   â”œâ”€â”€ text_generation_improved.yaml # Improved recipe generation configuration
â”‚   â””â”€â”€ recipe_prompt.txt             # Text prompt template for recipe generation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Original datasets for both projects
â”‚   â”œâ”€â”€ processed/                    # Preprocessed data files
â”‚   â””â”€â”€ img/                          # Images for documentation
â”œâ”€â”€ docs/                             # Documentation files
â”œâ”€â”€ logs/                             # Training and evaluation logs
â”œâ”€â”€ models/                           # Saved model checkpoints
â”‚   â”œâ”€â”€ sentiment/                    # Sentiment analysis model
â”‚   â””â”€â”€ recipe/                       # Recipe generation model
â”œâ”€â”€ notebooks/                        # Jupyter notebooks for exploration
â”œâ”€â”€ scripts/                          # Utility scripts for automation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                         # Data loading and preprocessing code
â”‚   â”‚   â”œâ”€â”€ dataset.py                # Base dataset functions
â”‚   â”‚   â”œâ”€â”€ sentiment_create_test_set.py       # Test set creation for sentiment analysis
â”‚   â”‚   â”œâ”€â”€ sentiment_create_balanced_test_set.py  # Balanced test set for sentiment
â”‚   â”‚   â”œâ”€â”€ prepare_gen_dataset.py    # Data preparation for text generation
â”‚   â”‚   â””â”€â”€ recipe_prepare_dataset.py # Recipe dataset preparation
â”‚   â”œâ”€â”€ model/                        # Model-related code
â”‚   â”‚   â”œâ”€â”€ generation/               # Additional generation model components
â”‚   â”‚   â”œâ”€â”€ sentiment_model_loader.py # Code to load sentiment models and tokenizers
â”‚   â”‚   â”œâ”€â”€ sentiment_train.py        # Sentiment model training
â”‚   â”‚   â”œâ”€â”€ sentiment_inference.py    # Sentiment model inference
â”‚   â”‚   â”œâ”€â”€ sentiment_evaluate.py     # Sentiment model evaluation
â”‚   â”‚   â”œâ”€â”€ sentiment_publish.py      # Publish sentiment model to HF Hub
â”‚   â”‚   â”œâ”€â”€ recipe_train.py           # Recipe model training
â”‚   â”‚   â”œâ”€â”€ recipe_evaluate.py        # Recipe model evaluation
â”‚   â”‚   â”œâ”€â”€ recipe_export_to_ollama.py # Export recipe model to Ollama
â”‚   â”‚   â”œâ”€â”€ recipe_merge_and_export_lora.py  # Merge LoRA adapters and export
â”‚   â”‚   â””â”€â”€ update_model_card.py      # Generate/update model cards (general utility)
â”‚   â”œâ”€â”€ utils/                        # Utility functions
â”‚   â”‚   â”œâ”€â”€ config_utils.py           # Configuration utilities
â”‚   â”‚   â”œâ”€â”€ recipe_formatter.py       # Format recipe outputs
â”‚   â”‚   â”œâ”€â”€ recipe_generator.py       # Recipe generation utilities
â”‚   â”‚   â””â”€â”€ recipe_prompts.py         # Recipe prompt templates
â”‚   â”œâ”€â”€ sentiment_demo.py             # Sentiment analysis demo
â”‚   â”œâ”€â”€ recipe_demo.py                # Recipe generation CLI demo
â”‚   â”œâ”€â”€ recipe_web_demo.py            # Recipe generation web UI demo
â”‚   â”œâ”€â”€ direct_recipe_test.py         # Direct testing of recipe generation
â”‚   â””â”€â”€ recipe_export_to_ollama_utils.py # Consolidated Ollama export utilities
â”œâ”€â”€ tests/                            # Automated tests
â”‚   â”œâ”€â”€ conftest.py                   # Pytest configurations
â”‚   â”œâ”€â”€ test_sentiment_model.py       # Tests for sentiment analysis
â”‚   â””â”€â”€ test_recipe_model.py          # Tests for recipe generation
â”œâ”€â”€ wandb/                            # Weights & Biases logging data
â”œâ”€â”€ GETTING_STARTED.md                # Getting started guide
â”œâ”€â”€ GETTING_STARTED_DEMO_1.md         # Demo 1 guide with testing documentation
â”œâ”€â”€ GETTING_STARTED_DEMO_2.md         # Demo 2 guide
â”œâ”€â”€ AI_TESTING_IDEAS.md               # Ideas for AI model testing
â”œâ”€â”€ DATASET_INSTRUCTIONS.md           # Dataset preparation instructions
â”œâ”€â”€ model_card.md                     # Model card template
â”œâ”€â”€ requirements.txt                  # Project dependencies
â”œâ”€â”€ setup_env.sh                      # Environment setup script
â”œâ”€â”€ Makefile                          # Automation of common tasks
â””â”€â”€ LICENSE                           # Project license
```

## Explanation:

This project is organized into two main applications:

1. **Sentiment Analysis (DistilBERT)**: A classification task that analyzes movie reviews
   - Training: `src/model/sentiment_train.py`
   - Inference: `src/model/sentiment_inference.py`  
   - Demo: `src/sentiment_demo.py`
   - Tests: `tests/test_sentiment_model.py`

2. **Recipe Generation (TinyLlama)**: A text generation task that creates recipes
   - Training: `src/model/recipe_train.py`
   - Evaluation: `src/model/recipe_evaluate.py`
   - Demos: 
     - CLI: `src/recipe_demo.py`
     - Web UI: `src/recipe_web_demo.py`
   - Tests: `tests/test_recipe_model.py`
   - Deployment: Various export utilities for Ollama

The project uses:
- **YAML configurations** in the `config/` directory for model parameters
- **Weights & Biases** for experiment tracking
- **Pytest** for automated testing
- **Hugging Face** and **Ollama** for model deployment

## Integrations
Here are some integrations and tools that can be used for free or on low-cost/free tiers:

a. Model & Data Libraries
Hugging Face Transformers:
Use this library to load pretrained models and tokenizers (e.g., AutoModelForCausalLM and AutoTokenizer). It also provides the Trainer API which simplifies training loops.

Hugging Face Datasets:
Efficiently load, preprocess, and manage datasets.

b. Testing and QA
Pytest:
A robust testing framework for Python. Write unit tests for data processing, model loading, and training routines.

Coverage Tools:
Use tools like pytest-cov to track test coverage.

Mocking Libraries:
For parts of your code that require heavy resources (like the full fine-tuning loop), you can use mocking (with libraries like unittest.mock) to simulate behavior on a small scale.

c. Configuration and Experiment Management
Configuration Management:
Use YAML/JSON configuration files. Libraries like PyYAML can help you read configurations into your Python scripts.

Logging and Monitoring:

TensorBoard: Integrated with PyTorch or TensorFlow (if you use them indirectly via Hugging Face) to track metrics.
Weights & Biases (wandb): The free tier is often sufficient for logging experiments and tracking hyperparameters.
d. Version Control and CI/CD
Git:
Use Git for version control. GitHub or GitLab offer free private repositories.

GitHub Actions:
Set up a simple CI pipeline to run your tests automatically on every commit or pull request.

e. Compute Resources
Local CPU/Low-end GPU:
Fine-tuning a 14B parameter model is resource intensive, so for testing and learning:

Use a smaller model version or a subset of your data for rapid iteration.
Leverage gradient accumulation or mixed precision training to simulate training on limited hardware.
Cloud Platforms:
Consider free tiers from Google Colab, Kaggle Kernels, or free trial credits from cloud providers. These can be useful for initial experiments without incurring high costs.

Docker (Optional):
Containerize your environment to ensure reproducibility and ease deployment, though this is optional if your budget is very low.

## ðŸ”„ Version Control & Collaboration

### Getting Started with this Repository
